# -*- coding: utf-8 -*-
"""Llama_Model_for_Retail_Business.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xeMEcde5VDx2y5mnm0_vnEAW3qSlnOQL

#  QnA Chatbot for Chocolate Retail Business | Powered by LLaMA2

This project builds a custom chatbot designed to assist customers of a **retail business specializing in chocolates**, leveraging the **LLaMA2-7B-Chat** model for intelligent, context-aware responses.

The model is fine-tuned using **QLoRA** (LoRA adapters + 4-bit quantization), enabling efficient training within a Google Colab environment while maintaining high performance and relevance to domain-specific queries.

---
### üìä Dataset
- The dataset consists of **1000 QnA pairs** tailored specifically for a chocolate-focused retail business.  
- Each entry reflects realistic customer interactions on topics such as:
  - Taste profiles
  - Pricing and offers
  - Shipping/delivery
  - Gifting and packaging
  - Usage, storage, and dietary information

- Data was generated and refined using AI tools (e.g., ChatGPT), then formatted for supervised fine-tuning.

- Structure:
  - Stored in plain `.txt` files, where each QnA block is separated by two newlines.
  - Split into:
    -  **800 training samples**
    -  **100 validation samples**
    -  **100 test samples**

- These are preprocessed into the Hugging Face `Dataset` format and used in conjunction with `trl.SFTTrainer` for QLoRA fine-tuning.

---

###  Key Features:
- Uses **LLaMA2-7B-Chat** model
- Fine-tuned with **QLoRA** for low-memory training
- Supports **fallback response** for unknown or unclear inputs
- Model trained on **custom chocolate-related QnA data**
- Exports ready-to-use model for chatbot deployment

## Libraries
"""

#Install Required Libraries
!pip install -U transformers accelerate peft bitsandbytes
!pip install -U git+https://github.com/huggingface/trl
!pip install git+https://github.com/huggingface/trl.git@main
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
import torch
from datasets import Dataset
from trl import SFTTrainer
import torch
from torch.nn import CrossEntropyLoss
import math
from random import choice
import re
from peft import PeftModel

"""##Dataset"""

# üîÅ Helper function to load QnA blocks from .txt file
def load_qna(path):
    with open(path, "r", encoding="utf-8") as f:
        blocks = f.read().strip().split("\n\n")  # Split on double newlines
    return [{"text": block.strip()} for block in blocks]  # Wrap each block as a dict

# Load and format custom chocolate-retail QnA dataset
# Each .txt file contains realistic customer interactions
# Format: [INST] question [/INST] answer
# Files: Train.txt, Validation.txt, Test.txt

train_dataset = Dataset.from_list(load_qna("/content/Train.txt"))
val_dataset   = Dataset.from_list(load_qna("/content/Validation.txt"))
test_dataset  = Dataset.from_list(load_qna("/content/Test.txt"))

# Preview dataset stats
print("üìä Training Samples:", len(train_dataset))
print("üìä Validation Samples:", len(val_dataset))
print("üìä Test Samples:", len(test_dataset))

# Show one sample from each
print("\nExample Training Sample:\n", train_dataset[0]["text"])
print("\nExample Validation Sample:\n", val_dataset[0]["text"])
print("\nExample Test Sample:\n", test_dataset[0]["text"])

"""## Model building"""

# Authenticate with Hugging Face Hub to access gated models (e.g., LLaMA 2)

#Generate own token from Hugging Face as the token is confidential and comment out the below code


#login(token=input("Enter your HF token: "))

#  Load the base pre-trained LLaMA2-7B model from Hugging Face
model_name = "meta-llama/Llama-2-7b-hf"

# ‚öôÔ∏è Configure 4-bit quantization using BitsAndBytes for efficient memory usage
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Enables 4-bit loading
    bnb_4bit_compute_dtype=torch.float16  # Use float16 for matrix multiplications
)

#  Load tokenizer for the LLaMA2 model
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# Set padding token to EOS (required for some models during generation)
tokenizer.pad_token = tokenizer.eos_token

# Load the quantized LLaMA2 model with device mapping (e.g., to GPU)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # Automatically maps model layers to available GPU(s)
    quantization_config=bnb_config  # Apply 4-bit quantization config
)

#  Define LoRA (Low-Rank Adaptation) configuration
lora_config = LoraConfig(
    r=16,                             # Rank of the low-rank matrices
    lora_alpha=32,                   # Scaling factor for LoRA weights
    target_modules=["q_proj", "v_proj"],  # Target specific attention projection layers
    lora_dropout=0.05,               # Dropout applied to LoRA layers during training
    bias="none",                     # Don't train bias parameters
    task_type=TaskType.CAUSAL_LM     # Set task type for causal language modeling
)

#  Apply LoRA adapters to the base model
model = get_peft_model(model, lora_config)

# Define training configuration using Hugging Face's TrainingArguments

training_args = TrainingArguments(
    output_dir="./llama2-output",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    logging_steps=10,
    save_total_limit=2,
    save_steps=100,
    fp16=True,
    report_to="none"
)

# Fine-tune the model using TRL's SFTTrainer (Supervised Fine-Tuning)
# Uisng formatting_func=lambda x: x["text"] because the dataset is a list of dictionaries like {"text": "[INST] question [/INST] answer"}.


trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    peft_config=lora_config,
    args=training_args,
    formatting_func=lambda x: x["text"]
)

# Model Training
trainer.train()

"""## Model Evaluation"""

# Sample prediction of Test Dataset
sample = choice(test_dataset)["text"]

#  Tokenize the sample prompt and move it to the GPU
inputs = tokenizer(sample, return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=100)


print("üìå Prompt:\n", sample)
print("\nüì¶ Response:\n", tokenizer.decode(outputs[0], skip_special_tokens=True))

# Function to compute average loss and perplexity on a dataset

def compute_perplexity(model, tokenizer, dataset, max_length=512):
    model.eval()
    losses = []

    for sample in dataset:
        text = sample['text']
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length)
        input_ids = inputs["input_ids"].to("cuda")
        with torch.no_grad():
            outputs = model(input_ids, labels=input_ids)
        loss = outputs.loss
        losses.append(loss.item())

    avg_loss = sum(losses) / len(losses)
    ppl = math.exp(avg_loss)
    return avg_loss, ppl

#  Evaluate on validation set

val_loss, val_ppl = compute_perplexity(model, tokenizer, val_dataset)
print(f"üìä Validation Loss: {val_loss:.4f}")
print(f"üìâ Validation Perplexity: {val_ppl:.2f}")

# Function to compute Exact Match (EM) accuracy on the test dataset

def exact_match_accuracy(model, tokenizer, dataset):
    model.eval()
    match = 0

    for sample in dataset:
        prompt = sample["text"]
        if "[/INST]" not in prompt:
            continue
        split_idx = prompt.index("[/INST]") + len("[/INST]")
        expected_answer = prompt[split_idx:].strip()

        input_prompt = prompt[:split_idx]
        inputs = tokenizer(input_prompt, return_tensors="pt", truncation=True, max_length=512).to("cuda")
        with torch.no_grad():
            output = model.generate(**inputs, max_new_tokens=100)
        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        predicted_answer = generated_text[split_idx:].strip()

        if predicted_answer == expected_answer:
            match += 1

    return match / len(dataset) * 100

#  Evaluate Exact Match on Test Set

em_score = exact_match_accuracy(model, tokenizer, test_dataset)
print(f"‚úÖ Exact Match Accuracy on Test Set: {em_score:.2f}%")

print(f"üìä Validation Loss: {val_loss:.4f}")
print(f"üìâ Validation Perplexity: {val_ppl:.2f}")
print(f"‚úÖ Exact Match Accuracy on Test Set: {em_score:.2f}%")

"""# Testing the QnA Bot"""

# Set padding token to the same as end-of-sequence (EOS) token

tokenizer.pad_token = tokenizer.eos_token

# Adding Fall back message



#  Detect gibberish input (very few real words or too many non-alphabet characters)
def is_gibberish(text):
    # If fewer than 3 real words or high non-alpha content, call it gibberish
    words = re.findall(r'\b[a-zA-Z]{4,}\b', text)
    non_alpha_ratio = sum(1 for c in text if not c.isalpha() and c != ' ') / max(len(text), 1)
    return len(words) < 2 or non_alpha_ratio > 0.4


#  Main: Ask the chatbot and return a response
def ask_bot(question, max_new_tokens=100):
    fallback_msg = "I'm not sure how to answer that. Please contact the seller for more help."

    # Handle garbage or unclear user input
    if is_gibberish(question):
        return fallback_msg

    # Format user input into prompt for LLaMA2
    prompt = f"[INST] {question.strip()} [/INST]"
    inputs = tokenizer(prompt, return_tensors="pt", return_token_type_ids=False, truncation=True, max_length=512).to("cuda")

    # Generate response from model
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.05
        )


    # Postprocess the model output
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    # Clean up
    decoded = decoded.replace(prompt, "").strip()
    decoded = re.sub(r"\[/?INST\]", "", decoded)
    decoded = re.sub(r"<s>|</s>", "", decoded).strip()


    # Check if model response is valid, or fallback
    is_empty = decoded == ""
    is_disclaimer = any(x in decoded.lower() for x in [
        "language model", "as an ai", "i cannot", "i do not have access"
    ])
    looks_like_question = decoded.endswith("?") and not decoded.lower().startswith("yes")
    is_non_alpha = sum(c.isalpha() for c in decoded) / max(len(decoded), 1) < 0.4

    if is_empty or is_disclaimer or is_non_alpha or looks_like_question:
        return fallback_msg

    return decoded

#  Test chatbot for 10 interactions
for i in range(10):
    user_input = input(f"üßë‚Äçüíª You ({i+1}/10): ")

    if user_input.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break

    response = ask_bot(user_input)

    print("ü§ñ Bot:", response, "\n")

"""# Saving the Model"""

# Merge LoRA adapter into base model
merged_model = model.merge_and_unload()

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Save merged model + tokenizer to Drive
save_path = "/content/drive/MyDrive/llama2-final"
merged_model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

"""## Generate Requirement File"""

# Generating Requiremnt.txt File
!pip freeze > requirements.txt
print("‚úÖ requirements.txt generated with all installed packages.")

# Generating Clean Requiremnt.txt File
!pip freeze | grep -E 'transformers|torch|accelerate|peft|bitsandbytes' > requirements_minimal.txt

"""# Conclusion

In this notebook, I successfully fine-tuned the LLaMA2-7B-Chat model using QLoRA to build a custom chatbot for a chocolate-based retail business. I started by preparing a domain-specific QnA dataset and formatting it for training. The model was trained on Colab using a memory-efficient setup, evaluated thoroughly, and finally saved for deployment.

To make the bot more user-friendly, I also added fallback logic to handle gibberish or irrelevant questions.

### Model Accuracy
After training, the model achieved an Exact Match Accuracy of 100% on the test set, which means it perfectly reproduced the expected answers for all test cases. This shows that the model has learned the dataset extremely well and is ready for real-world queries.
"""

