{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7HlrQGnDtI-"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xvh-7sqfXmK2",
    "outputId": "cb971e65-b1ef-45bd-bcd4-a4de2565cc0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for trl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#  Install required packages\n",
    "!pip install -U -q transformers accelerate peft bitsandbytes\n",
    "!pip install -U -q git+https://github.com/huggingface/trl.git\n",
    "!pip install -q evaluate\n",
    "!pip install -q sacrebleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nMiXn9TYsADd"
   },
   "outputs": [],
   "source": [
    "#  Imports\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvIwD89NDre7"
   },
   "source": [
    "##Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECcA2f9kEFXr",
    "outputId": "36ff2498-6b4c-4bf4-cb78-b94d45a074bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 800\n",
      "Val samples: 100\n",
      "Example: Q: How do I get in touch with the company?\n",
      "A: You can email us directly at unthealthandfood@gmail.com for any help or questions.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Load QnA dataset from plain .txt files\n",
    "def load_qna(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        blocks = f.read().strip().split(\"\\n\\n\")  # Two newlines = one QnA pair\n",
    "    return [{\"text\": block.strip()} for block in blocks]\n",
    "\n",
    "train_dataset = Dataset.from_list(load_qna(\"Train.txt\"))\n",
    "val_dataset   = Dataset.from_list(load_qna(\"Validation.txt\"))\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Val samples:\", len(val_dataset))\n",
    "print(\"Example:\", train_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tqEGi1NfD1W2"
   },
   "outputs": [],
   "source": [
    "# ✅ Load GPT-2-Large with 4-bit quantization\n",
    "model_name = \"gpt2-large\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbRqLvuLEUeA"
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "89eaf9ce54eb4aeea6127c8974d09988",
      "1789b05bc76b4ac99df2536ee0aec2e5",
      "ddb3ab78cba64009bbd198ba684392cc",
      "727eda797ea94289ab3bb479cc03026d",
      "288bb9ce0d984c40aefd2f98bc4f9743",
      "d4ada497c14f42059066cdcabbfbf5f6",
      "5700c72aabcb4097b9703ec86bfaacf7",
      "34a49d3bbc9a4df0a3e121d0611894ab",
      "ace4db750484498cb0a872fd0ebde41a",
      "6e8a1b81c0e7468589acac0fab5a44cc",
      "ba7511e4db474461ab5ecfe03e842096",
      "bd62ab60f1194d199b9554019e5d9c47",
      "179d5b1c044b4829a50506b8eed008d0",
      "b10e9d428198445f8bb7d53626503429",
      "3bdce1abfced4802bee48bd3df027e74",
      "1efe50281bfa463c9cc82ee20532f96a",
      "7336a903a3fa4c3f9e46098bcdb886cc",
      "92584227469040a7b74699024654b1ea",
      "5783e197c23649e8868bb7a5ec8b1e7c",
      "36945048ca7648e787b18c98eebbee80",
      "40d0faa5323649d5a3c640705329b1a9",
      "843f92d0978b4d4bb743b889922a6cec",
      "d625011e0d5d4988b0488ca825cf4084",
      "8525dcc1ead944cc865b7fadc10128cc",
      "0a25ba46419d4d4d94461866fc93e228",
      "a0044a2fedb6430a80cc896e908ab5e0",
      "2769334b28e742ddb9ce9c2220361cf5",
      "6d26e381afb54017bdd60b8a72a8bbaf",
      "e10f90e5c833481a8eab42c78c5f4170",
      "c00f3d379f234d119d66a566726ccd0f",
      "bfa255c3db01444394630f9d5102bf71",
      "a00e6579a45b4cc4a03e43ba8ddb62d2",
      "dae55f8b187f49348e84873f943b8322",
      "a7512d8dbb15458f81f5f7e9058e5b58",
      "cade613ff4584ea9a2e61c81e758c1f4",
      "35b4c7b6056e4b3fa0adee3833b52d0d",
      "702b999d60674181847609ef0f71b7c1",
      "3c7ecbc5542a428faf6c617750c14455",
      "e6f70250c32a4fa09bc4886732f71b5c",
      "1ac13a3b41374fb3a818950d83d83a1a",
      "6eb75ff2aa5141dc838fef9274b55880",
      "d2d85262ebdd4e9e834085b7616e6554",
      "7d91aad5d93e4f5eabf6e575d4806f1c",
      "19e29179d8624775aea67c4bf6138876",
      "5d552145534c4d2299febd33f1df54bd",
      "86641e5f70214dd2bc4bcde5c846a6e0",
      "0ecc48056462449191b8ddd9aed54eef",
      "169d67b89d344983b88e5d27d87f7553",
      "8a9ecc61dba24168b62fe2bb06e85be9",
      "590c26d50225477987499d40d8ef35a6",
      "e3219de389dc4a4ca457370b078ffec4",
      "dce8a6b827b04ed5bcda60970526a856",
      "bb4df6b297d24f12a9b734c5eb34c89b",
      "d0c417ebfa404becac601f85a0a939c5",
      "673a40e1713048b6b3ad287bc4724aed",
      "84a0e4acc6434cc7a20d6c969af1d5ee",
      "33fd67615dcf4058b4975bf20757c2f6",
      "2fcc57f440bf46418278ef4faa1864dd",
      "99304cddc2214deda6a433c605eac71e",
      "ca48aa1c4ed945debca19bf67835e0f5",
      "b46db5277f8d4d29b0d094259c5d6794",
      "d8c974799d5847d9bbedb70bea836793",
      "eee0291c0a6e4dada8157ecff8a09a95",
      "c10dfbe638624501a8f6ce2571a27093",
      "fcfa71773f2449248ec13ff78adbd225",
      "ce9bcadab2ba48a8b5d0f58f30d0d6b4",
      "32b381bcfc8d4c51a4322c0c3ae35102",
      "a97cd042210c442e962121f39e9377f4",
      "fd1771b8ee3745e78c8197d538fbfb56",
      "457dcb64855842399538c33020c3ab96",
      "6fe68c4c2276417d934c76dfe9728371",
      "194ee8b6b32949208f7e679db7eabd25",
      "a626e25e3e76456b88583501e471edf7",
      "5bc41a1ac08c41d6bc33adca3a9918e4",
      "2e07854d62a943a49f47a486872401f3",
      "0e6f8b48e37340beb50e7341dc1b382e",
      "fd504bdf64ca48eb99ff2c97189414b0"
     ]
    },
    "id": "0CQCNXyHYIKt",
    "outputId": "d141d752-f5ca-4401-bef8-fa8281caecf6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89eaf9ce54eb4aeea6127c8974d09988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd62ab60f1194d199b9554019e5d9c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d625011e0d5d4988b0488ca825cf4084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7512d8dbb15458f81f5f7e9058e5b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d552145534c4d2299febd33f1df54bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a0e4acc6434cc7a20d6c969af1d5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b381bcfc8d4c51a4322c0c3ae35102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# avoiding padding issues\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eD-MlfvwWIpy"
   },
   "outputs": [],
   "source": [
    "#  Apply LoRA for QLoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QlyYgP6QWwUk"
   },
   "outputs": [],
   "source": [
    "#  Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-qlora-output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    save_steps=100,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414,
     "referenced_widgets": [
      "1155a5bfe5c641dfb87e0102a18e0d61",
      "6d85149fc44e4ddd94465e5957881ce3",
      "c4229965e9834f45b071582d4a756f25",
      "78072444c2784e8e9a0571ece6269d3c",
      "9323e3f3f8944a5a9a99c865a646a2d1",
      "fbac3df5921c4fb9afe42a91cc2fe1b9",
      "bf44f6417ba44d898bba01e3f00339f7",
      "822edf2e6de04e5784d70e6e213d34b0",
      "038746e99b744ec1a6cd7423659136a6",
      "a7cf2dfe127b44f5a11cbc1355b5a95d",
      "16a9c1a8ee604ca88a6cff791c42d1c0",
      "44fd867511cf4046a3091fcf7a93dc1d",
      "29ec89727d16470898612661eabf37c6",
      "626a8b86e90b48d8991051c32b254f6a",
      "a73a9c6ff6e548a9aece701508cc8574",
      "8b86f9cc86654b5996e031fc70ec0b52",
      "06d0f4bf1ec245e3b7eec20f9d54a8e2",
      "da3b597f20ed4aef80b54e1b5d8892b1",
      "168690282fc94e548bbcbac2ce4cd721",
      "58ed895177ba41c2a9ac5ed452f461d4",
      "072973a18c9449b1a2b41da5c1ab769c",
      "f1aa7056276c472ca4252e01644c8f9d",
      "9984c90507d648169064d47645b6c2c3",
      "364a533e3ebc4b63a760a1a43fea3d29",
      "6ca8db6ee1b144ea99c1f473645f3d30",
      "5065fea64e504b06bb2575451f3d6954",
      "4c6c026c73dc479dbe6c76b80014a989",
      "af214cca0cbd4b3484a06cd21cbb0ad2",
      "8aee19d7935545019931d677e20e739a",
      "53d8ab54bdea4673a7411ad4af7fe3d3",
      "9fd6c12b5dbf4ecf839790e48fafe9ab",
      "13f5252578d44beaa2f2208a998483d3",
      "abe6036ebe4b4e25b5cd25ba70f417c5",
      "774ec8cb2e6e4505b0f616e4b572a9e0",
      "951e42ebd682412d9929269999965411",
      "843958f14035460894e32c16a0c49f80",
      "bb4a613d2ae845bea932389876ff56ef",
      "08becd0588c54448aaf4f524ec23306f",
      "e6012cc71a784a4a9b7d493f5b19e5c5",
      "dd41e9a9161346deaf5fe01da3d7414e",
      "0768f591fe1c44cea6a0f0731264a64e",
      "fdb5a09ed3da4bada4601b4107a702a3",
      "b4d8141b079a4ded898010509eacd9c7",
      "ae658c37588947dc967c97e06b81c92b",
      "4d0d61d157c24657b3d7453e751cc7a8",
      "6c60926631974fe899435c60fcc1630f",
      "123fd0523858471789c03bdaed86f37b",
      "12a4e656b9b94bd2838bce3f69595c3c",
      "18f20f2c55054d88a05c29fd00aa68c5",
      "18703efb209f41b498049951c77d1eea",
      "74e55f8ddfc3481e9681db663b116039",
      "f22edcdd238a49b9ab488022ae28e778",
      "f9bae735b5b745baa6312337dbe4126b",
      "be87838d44b64d5d8b2e20b5a28d22e4",
      "415ab50a8dd044db8f18b7eca42be912",
      "acdb18914e6343a58dc7c3de733568b4",
      "b776b17912e44114b039bb0a5662b85e",
      "76a55bf1dbf04f8992ef6c78cdcda8ff",
      "c058bb91cf0740cc8f4a3d5e2e293b1f",
      "1c8e52b2618a4eb6a07d8568434a2bc6",
      "0c57a705a46e461589e280161a7c8f39",
      "2fbb5894bd3b42d283e650f84a909fae",
      "d68363661c714e438e9f53af2de08e28",
      "6660d0b981834788b13dd1816e5ee4e0",
      "f30a863b91ed4b878d9203ac1d5355ba",
      "1c58deb362d34854924fdd8bb81a9d13",
      "c0711e569bc1452f93cfcdd4312adf4a",
      "0219bc9750ba4795927d0e7bd0c3f9c6",
      "901a1e514e2b464c845f39e4556b5d13",
      "8a260691747e4ed4acbc26ffebf8d834",
      "0d51fe62c3b3400fbaade30dc4405a21",
      "1aebacc7340542aaa062558126f64342",
      "a56436e73d6c40c3ae30fb7600edebd7",
      "0e3fe371a324428d9feda6b5285b0828",
      "90f7aaeb00674ada9f27407f6c42ee58",
      "cb670f5c4b714dcb81bf88bb436d431e",
      "fefb813326b449eea25e2f76f728dee4",
      "16f64ec8f95f4e83be4b8fd968c24a83",
      "47c64da81cd6494897a6b627b16a93e4",
      "0f2a9debff1b42cfa249d52a27fcfa7d",
      "fbcc111ff0a44ef3909066acb1c7478c",
      "e68a07231ec84dc988f63a49518a81f2",
      "c89d4f6ed3c446528e99104b4b39e811",
      "07db5c6b2868433b8553da183c2b8327",
      "74b4297a38b24289b01e6b34ed7caa6b",
      "1751fef3c84341409d831eed4ecb01fa",
      "17b682df4eb143409c1719ade38aa0f6",
      "0379a8e0423b4719b04552af1e6ba753"
     ]
    },
    "id": "s3CIlxGEW-bm",
    "outputId": "120bbb7b-8a09-4de0-e087-c87ab735c83f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'gpt2-large' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1155a5bfe5c641dfb87e0102a18e0d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fd867511cf4046a3091fcf7a93dc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9984c90507d648169064d47645b6c2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774ec8cb2e6e4505b0f616e4b572a9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0d61d157c24657b3d7453e751cc7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdb18914e6343a58dc7c3de733568b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0711e569bc1452f93cfcdd4312adf4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f64ec8f95f4e83be4b8fd968c24a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "#  Fine-tune using SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=lambda x: x[\"text\"],\n",
    "    peft_config=lora_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZDvQD8JjXHHm",
    "outputId": "16ea88cb-6b3c-4245-beee-6b86b90bcbfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 07:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.825900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.864800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.726600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.492700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.253700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.7549880544344584, metrics={'train_runtime': 484.977, 'train_samples_per_second': 4.949, 'train_steps_per_second': 0.619, 'total_flos': 444583053250560.0, 'train_loss': 0.7549880544344584})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1suo3FNXF19G"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e6btlDADuls6",
    "outputId": "0aa6832a-2a27-4350-987f-78b65b13f687"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 01:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1686\n",
      "📉 Validation Perplexity: 1.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EM:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:   1%|          | 1/100 [00:01<03:00,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:   2%|▏         | 2/100 [00:03<02:29,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:   3%|▎         | 3/100 [00:04<02:30,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:   4%|▍         | 4/100 [00:06<02:19,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:   5%|▌         | 5/100 [00:07<02:19,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:   6%|▌         | 6/100 [00:09<02:23,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:   7%|▋         | 7/100 [00:10<02:15,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:   8%|▊         | 8/100 [00:11<02:15,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:   9%|▉         | 9/100 [00:13<02:09,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  10%|█         | 10/100 [00:14<02:15,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  11%|█         | 11/100 [00:16<02:01,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  12%|█▏        | 12/100 [00:17<01:57,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  13%|█▎        | 13/100 [00:19<02:10,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  14%|█▍        | 14/100 [00:21<02:22,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  15%|█▌        | 15/100 [00:22<02:22,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  16%|█▌        | 16/100 [00:24<02:17,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  17%|█▋        | 17/100 [00:25<02:10,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  18%|█▊        | 18/100 [00:27<02:12,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  19%|█▉        | 19/100 [00:29<02:21,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  20%|██        | 20/100 [00:31<02:17,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  21%|██        | 21/100 [00:33<02:20,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  22%|██▏       | 22/100 [00:34<02:03,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  23%|██▎       | 23/100 [00:36<02:10,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  24%|██▍       | 24/100 [00:38<02:09,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  25%|██▌       | 25/100 [00:39<02:05,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  26%|██▌       | 26/100 [00:41<02:02,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  27%|██▋       | 27/100 [00:42<01:47,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  28%|██▊       | 28/100 [00:43<01:48,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  29%|██▉       | 29/100 [00:45<01:48,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  30%|███       | 30/100 [00:47<01:52,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  31%|███       | 31/100 [00:48<01:41,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  32%|███▏      | 32/100 [00:50<01:50,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  33%|███▎      | 33/100 [00:51<01:44,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  34%|███▍      | 34/100 [00:53<01:37,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  35%|███▌      | 35/100 [00:54<01:34,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  36%|███▌      | 36/100 [00:56<01:42,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  37%|███▋      | 37/100 [00:58<01:40,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  38%|███▊      | 38/100 [00:59<01:33,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  39%|███▉      | 39/100 [01:00<01:29,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  40%|████      | 40/100 [01:02<01:25,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  41%|████      | 41/100 [01:03<01:21,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  42%|████▏     | 42/100 [01:05<01:29,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  43%|████▎     | 43/100 [01:06<01:25,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  44%|████▍     | 44/100 [01:08<01:24,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  45%|████▌     | 45/100 [01:10<01:28,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  46%|████▌     | 46/100 [01:11<01:19,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  47%|████▋     | 47/100 [01:12<01:15,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  48%|████▊     | 48/100 [01:14<01:15,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  49%|████▉     | 49/100 [01:15<01:19,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  50%|█████     | 50/100 [01:17<01:13,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  51%|█████     | 51/100 [01:18<01:12,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  52%|█████▏    | 52/100 [01:20<01:18,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  53%|█████▎    | 53/100 [01:22<01:18,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  54%|█████▍    | 54/100 [01:23<01:13,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  55%|█████▌    | 55/100 [01:25<01:12,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  56%|█████▌    | 56/100 [01:26<01:10,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  57%|█████▋    | 57/100 [01:28<01:01,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  58%|█████▊    | 58/100 [01:29<01:00,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  59%|█████▉    | 59/100 [01:31<01:00,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  60%|██████    | 60/100 [01:33<01:07,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  61%|██████    | 61/100 [01:34<01:05,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  62%|██████▏   | 62/100 [01:36<01:04,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  63%|██████▎   | 63/100 [01:37<00:58,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  64%|██████▍   | 64/100 [01:39<00:51,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  65%|██████▌   | 65/100 [01:40<00:49,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  66%|██████▌   | 66/100 [01:41<00:49,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  67%|██████▋   | 67/100 [01:43<00:47,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  68%|██████▊   | 68/100 [01:45<00:51,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  69%|██████▉   | 69/100 [01:46<00:48,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  70%|███████   | 70/100 [01:48<00:44,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  71%|███████   | 71/100 [01:49<00:42,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  72%|███████▏  | 72/100 [01:51<00:42,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  73%|███████▎  | 73/100 [01:52<00:37,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  74%|███████▍  | 74/100 [01:54<00:38,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  75%|███████▌  | 75/100 [01:55<00:36,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  76%|███████▌  | 76/100 [01:56<00:34,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  77%|███████▋  | 77/100 [01:58<00:35,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  78%|███████▊  | 78/100 [02:00<00:34,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  79%|███████▉  | 79/100 [02:01<00:33,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  80%|████████  | 80/100 [02:03<00:30,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  81%|████████  | 81/100 [02:04<00:27,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  82%|████████▏ | 82/100 [02:05<00:25,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  83%|████████▎ | 83/100 [02:07<00:23,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  84%|████████▍ | 84/100 [02:08<00:23,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  85%|████████▌ | 85/100 [02:10<00:23,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  86%|████████▌ | 86/100 [02:12<00:21,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  87%|████████▋ | 87/100 [02:13<00:19,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  88%|████████▊ | 88/100 [02:15<00:18,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  89%|████████▉ | 89/100 [02:16<00:15,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  90%|█████████ | 90/100 [02:18<00:15,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  91%|█████████ | 91/100 [02:19<00:12,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  92%|█████████▏| 92/100 [02:20<00:11,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  93%|█████████▎| 93/100 [02:21<00:09,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  94%|█████████▍| 94/100 [02:23<00:08,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  95%|█████████▌| 95/100 [02:24<00:06,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  96%|█████████▌| 96/100 [02:26<00:05,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  97%|█████████▋| 97/100 [02:27<00:04,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  98%|█████████▊| 98/100 [02:29<00:03,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM:  99%|█████████▉| 99/100 [02:30<00:01,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating EM: 100%|██████████| 100/100 [02:32<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exact Match Accuracy on Validation Set: 96.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "#  Validation Loss\n",
    "eval_loss = eval_results[\"eval_loss\"]\n",
    "print(f\"Validation Loss: {eval_loss:.4f}\")\n",
    "\n",
    "#  Validation Perplexity\n",
    "perplexity = math.exp(eval_loss)\n",
    "print(f\"📉 Validation Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "#  Exact Match Accuracy on Validation Set\n",
    "def compute_exact_match(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Evaluating EM\"):\n",
    "        input_text = example[\"text\"]\n",
    "        if \"A:\" not in input_text:\n",
    "            continue\n",
    "        question, true_answer = input_text.split(\"A:\", 1)\n",
    "        prompt = question.strip() + \"A:\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract model's answer\n",
    "        pred_answer = generated.split(\"A:\")[-1].strip().split(\"\\n\")[0]\n",
    "        true_answer = true_answer.strip().split(\"\\n\")[0]\n",
    "\n",
    "        if pred_answer == true_answer:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "em_score = compute_exact_match(model, tokenizer, val_dataset)\n",
    "print(f\"✅ Exact Match Accuracy on Validation Set: {em_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swOQwYv3ul4C",
    "outputId": "70ec8fc7-b1ee-48c3-c4d6-83ff92e2e778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1686\n",
      "📉 Validation Perplexity: 1.18\n",
      "✅ Exact Match Accuracy on Validation Set: 96.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation Loss: {eval_loss:.4f}\")\n",
    "print(f\"📉 Validation Perplexity: {perplexity:.2f}\")\n",
    "print(f\"✅ Exact Match Accuracy on Validation Set: {em_score * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
