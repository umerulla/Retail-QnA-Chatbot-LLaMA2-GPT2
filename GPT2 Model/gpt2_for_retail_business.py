# -*- coding: utf-8 -*-
"""GPT2_for_Retail_Business.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O_RpT4DyEA21Xz94x9EyTRe9iHzIYlwE

## Libraries
"""

#  Install required packages
!pip install -U -q transformers accelerate peft bitsandbytes
!pip install -U -q git+https://github.com/huggingface/trl.git
!pip install -q evaluate
!pip install -q sacrebleu

#  Imports
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
from trl import SFTTrainer
import torch
import math
from tqdm import tqdm

"""##Dataset"""

# âœ… Load QnA dataset from plain .txt files
def load_qna(path):
    with open(path, "r", encoding="utf-8") as f:
        blocks = f.read().strip().split("\n\n")  # Two newlines = one QnA pair
    return [{"text": block.strip()} for block in blocks]

train_dataset = Dataset.from_list(load_qna("Train.txt"))
val_dataset   = Dataset.from_list(load_qna("Validation.txt"))

print("Train samples:", len(train_dataset))
print("Val samples:", len(val_dataset))
print("Example:", train_dataset[0]["text"])

# âœ… Load GPT-2-Large with 4-bit quantization
model_name = "gpt2-large"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

"""## Model building"""

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)
model = prepare_model_for_kbit_training(model)

# avoiding padding issues
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.eos_token_id

#  Apply LoRA for QLoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn", "c_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

#  Training setup
training_args = TrainingArguments(
    output_dir="./gpt2-qlora-output",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    fp16=True,
    logging_steps=10,
    save_total_limit=2,
    save_steps=100,
    report_to="none"
)

#  Fine-tune using SFTTrainer
trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    args=training_args,
    formatting_func=lambda x: x["text"],
    peft_config=lora_config
)

#Train model
trainer.train()

"""## Model Evaluation"""

# Evaluate on validation set
eval_results = trainer.evaluate()

#  Validation Loss
eval_loss = eval_results["eval_loss"]
print(f"Validation Loss: {eval_loss:.4f}")

#  Validation Perplexity
perplexity = math.exp(eval_loss)
print(f"ðŸ“‰ Validation Perplexity: {perplexity:.2f}")

#  Exact Match Accuracy on Validation Set
def compute_exact_match(model, tokenizer, dataset):
    model.eval()
    correct = 0
    total = 0

    for example in tqdm(dataset, desc="Evaluating EM"):
        input_text = example["text"]
        if "A:" not in input_text:
            continue
        question, true_answer = input_text.split("A:", 1)
        prompt = question.strip() + "A:"

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(**inputs, max_new_tokens=50)
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract model's answer
        pred_answer = generated.split("A:")[-1].strip().split("\n")[0]
        true_answer = true_answer.strip().split("\n")[0]

        if pred_answer == true_answer:
            correct += 1
        total += 1

    return correct / total if total > 0 else 0.0

em_score = compute_exact_match(model, tokenizer, val_dataset)
print(f"âœ… Exact Match Accuracy on Validation Set: {em_score * 100:.2f}%")

print(f"Validation Loss: {eval_loss:.4f}")
print(f"ðŸ“‰ Validation Perplexity: {perplexity:.2f}")
print(f"âœ… Exact Match Accuracy on Validation Set: {em_score * 100:.2f}%")